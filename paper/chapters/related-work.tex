\chapter{Related work}\label{sec:related}

Static program analysis is a hot topic in Computer Science research.  The Association for Computing Machinery (ACM) publishes several journals, such as Proceedings of the ACM on Programming Languages (PACMPL), Transactions on Programming Languages and Systems (TOPLAS), and Transactions On Software Engineering and Methodology (TOSEM), which are focused (at least in part) on static verification and type systems.  It should come as no surprise that there is a large body of research that is related to this thesis.  This Chapter references a tiny fraction of this body of work.  Section \ref{sec:related:effectiveness} calls upon past research to assert unquestionably the positive impact that static analysis has on the software development process.  Section \ref{sec:related:aftermarket} explores a line of research dedicated to inserting supplemental specifications into existing programming languages in order to improve the static checkability of those languages.  Lastly, Section \ref{sec:related:libclang} pays respect to the LLVM project which has enabled so much of the research for this thesis.

\section{On the Effectiveness of Static Analysis}\label{sec:related:effectiveness}

Studies have long shown that static analysis is an essential tool for developing high-quality software.  The high speed and low cost of this type of verification make it an economical method for finding faults in program code~\cite{staticanal, static-anal-experience}.  

Industry has taken this observation to heart.  Many companies have their own internal tools dedicated to statically checking code changes with a goal of detecting common mistakes and stylistic issues.  The Mozilla project is a good example of this --- since the early 2000s, Mozilla has used a fairly robust suite of internal tools specifically crafted for Mozilla's mostly C++ codebase.  Using these tools, every Pull Request into Mozilla Firefox is parsed and checked against a set of hand-written rules to detect and report common issues~\cite{mozilla-pork-blog, moz-wiki-static-anal}.  Much of this tooling was dedicated to detecting memory issues.  Of course, without modifying the grammar of C++, there are limitations in what can be easily checked statically by these tools.  Only a small subset of the problem could be effectively detected.  

More recently, Mozilla developed and began using a language called Rust which was designed with certain static analysis characteristics in mind.  The Rust language implements an innovative type system meant to formally track the ownership of objects in memory.  ``Rust's type system and runtime guarantee the absence of data races, buffer overflows, stack overflows, and access to uninitialized or deallocated memory''~\cite{rust-is-dope}.  A common sentiment in the Rust-language community is that even though the ``Borrow Checker'' (the part of the type system that enforces memory safety) seems complicated at first, seasoned Rust users learn to depend on it to help them reason through complicated programs~\cite{rust-lang-spec}.  Rust demonstrates that making a type system more expressive and more restrictive can improve both the static checkability of a programming language and also the help the users of those languages.  

\section{Aftermarket Type Systems --- Supplementing an Existing Language}\label{sec:related:aftermarket}

The idea of introducing new forms of type checking into an existing language to increase safety is nothing new.  As early as 1994 tools such as LCLint have existed which allow the programmer to write down specifications about their code that are not necessarily supported by the original language standard.  The LCLint tool can take program source code as well as a file containing supplemental specifications and perform static analysis that is more thorough and informed than could possibly be achieved based on the language standard alone~\cite{lclint-og}. 

A useful attribute of these supplemental static analysis tools is that they scale incrementally --- the programmer can use these tools to whatever extent they find helpful and can increase or reduce the amount of information they pass on to these tools as they see fit.  Since these specifications are opt-in, adding new forms of specification to a tool like LCLint is a straightforward way to expand the scope of the tool without breaking backwards compatibility.  As an example, in 1996, Evans \textit{et al}. added a few variable type annotations to LCLint such as \lstinline{not-null}, \lstinline{possibly-null}, and \lstinline{null}.  When used by the programmer, these annotations allow LCLint to check for certain kinds of errors relating to nullness and memory allocation~\cite{lclint-memory}.  Such modifications require zero action by the users that choose not to use them; if a variable is not annotated then LCLint will not try to check that variable.  However, as the user adds more annotations, LCLint is able to check more variables.  The amount of feedback LCLint is able to provide scales up and down with the amount of annotations in the code.  

In general, variable annotations like \lstinline{not-null} and \lstinline{possibly-null} are very similar in use to the existing system of type qualifiers in the C family of languages.  A canonical example of a type qualifier would be the C \lstinline{const} qualifier; a variable marked \lstinline{const} may be set once at declaration but never updated again (ignoring unsafe casts).  Type qualifiers and annotations like \lstinline{const} and \lstinline{not-null} have two benefits:  First, they declare the intent behind the code so that other programmers reading the code have a better idea of how it works.  Second, they dictate what the programmer can and cannot do with an identifier so that the compiler or other static checking tool can detect accidental misuse.  However, their use is entirely optional --- the programmer can choose to treat an identifier as \lstinline{const} or \lstinline{not-null} without actually adding the annotation~\cite{theory-of-qual}.

``A Theory of Type Qualifiers'' develops this concept in depth and explores the theoretical and practical concerns involved with using type qualifiers in a language~\cite{theory-of-qual}.  One of the most relevant observations to the work in this paper is that every type qualifier introduces a form of subtyping.  For all types $T$ and any qualifier $q$, either $T \leq q T$ or $q T \leq T$ depending on $q$\footnote{This notation is equivalent to the subset notation (i.e., $T \subseteq q T$ or $q T \subseteq T$).  We choose to use the less than operator because it matches the notation used by Foster in ``A Theory of Type Qualifiers''.}.  Here we notate $T$ qualified by $q$ as $q T$ and we notate $X$ is a subtype of $Y$ as $X \leq Y$.  $X \leq Y$ should be interpreted to mean that $X$ can be safely used whenever $Y$ is expected.  For example $\lstinline{int} \leq \lstinline{const int}$ because in any statement containing a $\lstinline{const int}$, one could safely substitute an $int$ however the reverse is not true.  In the same vein, $\lstinline{not-null char*} \leq \lstinline{char*}$ because any statement referencing a $\lstinline{char*}$ could safely be given a $\lstinline{not-null char*}$ instead~\cite{theory-of-qual}.  In this paper we will apply this concept to the type qualifiers introduced by funqual in order to argue for the correctness of funqual.  

\section{libClang and the Explosion of C++ Tooling}\label{sec:related:libclang}

C++ is difficult to parse~\cite{cpp-sucks, libclang-survey, mozilla-pork, parse-cpp}.  Years of language additions, the need for backwards compatibility, and the existence of a text-based preprocessor\footnote{In theory, preprocessing could be delegated to another tool like gcc.  In practice this generally leads to loss of information --- most notably with \lstinline{#include} directives obfuscating the locations of symbols in source code.} means that the language grammar is large and complicated.  As a result, even the simplest static analysis tools require a huge amount of complexity to do basic parsing of source code.  Up until relatively recently, many C++ language tools settled on doing a partial parse of the language using approximations and heuristics~\cite{libclang-survey}.  This method can lead to artificial constraints on the language or to incorrect interpretations of the source.  

As a result of the LLVM Compiler Infrastructure Project, we now have an excellent set of tools for working with code.  The Clang compiler is a fully featured compiler from the LLVM project that supports a wealth of C-family language standards including C++17.  The LLVM project also provides libClang which exposes a convenient API to the parser and the AST used by the Clang compiler.  libClang enables developers to create their own tools that build on top of Clang's C++ parser.  This means that developers of static analysis tools only need to focus on maintaining their project's contributions rather than supporting an entire parser/AST toolsuite~\cite{libclang-survey}.  Funqual is built using libClang and so the work done in this paper was only possible thanks to the work done by the LLVM Compiler Infrastructure Project.  
